{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sdg11 - trafficflow using induction loop sensors.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeP6xkRIBJco"
      },
      "source": [
        "# World Data League 2021\n",
        "## Notebook Template\n",
        "\n",
        "This notebook is one of the mandatory deliverables when you submit your solution (alongside the video pitch). Its structure follows the WDL evaluation criteria and it has dedicated cells where you can add descriptions. Make sure your code is readable as it will be the only technical support the jury will have to evaluate your work.\n",
        "\n",
        "The notebook must:\n",
        "\n",
        "*   üíª have all the code that you want the jury to evaluate\n",
        "*   üß± follow the predefined structure\n",
        "*   üìÑ have markdown descriptions where you find necessary\n",
        "*   üëÄ be saved with all the output that you want the jury to see\n",
        "*   üèÉ‚Äç‚ôÇÔ∏è be runnable\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QNcZrkVu9xf"
      },
      "source": [
        "## External links and resources\n",
        "Paste here all the links to external resources that are necessary to understand and run your code. Add descriptions to make it clear how to use them during evaluation.",
        "http://158.101.207.63:8080/data/ for data",
        "http://158.101.207.63:8080/models/ for models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJzSXXIYvxf9"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63ltgxp_rOpI"
      },
      "source": [
        "## Introduction\n",
        "Describe how you framed the challenge by telling us what problem are you trying to solve and how your solution solves that problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp34gOznrwrq"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8rCpNajszur"
      },
      "source": [
        "## Development\n",
        "Start coding here! üë©‚Äçüíª\n",
        "\n",
        "Don't hesitate to create markdown cells to include descriptions of your work where you see fit, as well as commenting your code.\n",
        "\n",
        "We know that you know exactly where to start when it comes to crunching data and building models, but don't forget that WDL is all about social impact...so take that into consideration as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpmOXABz-Jug"
      },
      "source": [
        "import sys\n",
        "import warnings\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras.models import Model, load_model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers.recurrent import LSTM, GRU\n",
        "from keras.models import Sequential\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import pydot_ng as pydot\n",
        "import datetime as dt\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afB4W0KnutpV"
      },
      "source": [
        "traffic_flow_2016 = pd.read_hdf(\"../predicting-traffic-flow/data/traffic_flow_2016.hdf\")\n",
        "traffic_flow_2017 = pd.read_hdf(\"../predicting-traffic-flow/data/traffic_flow_2016.hdf\")\n",
        "traffic_flow_2018 = pd.read_hdf(\"../predicting-traffic-flow/data/traffic_flow_2016.hdf\")\n",
        "traffic_flow_2019 = pd.read_hdf(\"../predicting-traffic-flow/data/traffic_flow_2016.hdf\")\n",
        "\n",
        "# Create groups by entity_id\n",
        "def preprocess_data(train_df, test_df):    \n",
        "    split_train = train_df[['entity_id','dateobservedfrom', 'intensity']]\n",
        "    split_test = test_df[['entity_id','dateobservedfrom', 'intensity']]\n",
        "\n",
        "    train_d = split_train.groupby(['entity_id'])\n",
        "    test_d = split_test.groupby(['entity_id'])\n",
        "    \n",
        "    return train_d, test_d\n",
        "\n",
        "train_df, test_df = preprocess_data(\n",
        "    train_df=pd.concat([traffic_flow_2016, traffic_flow_2017, traffic_flow_2018]), \n",
        "    test_df=traffic_flow_2019)\n",
        "\n",
        "del traffic_flow_2016, traffic_flow_2017, traffic_flow_2018, traffic_flow_2019"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4jPpcKOutZ5"
      },
      "source": [
        "def get_lstm(units):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units[1], input_shape=(units[0], 1), return_sequences=True))\n",
        "    model.add(LSTM(units[2]))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units[3], activation='sigmoid'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_gru(units):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(GRU(units[1], input_shape=(units[0], 1), return_sequences=True))\n",
        "    model.add(GRU(units[2]))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units[3], activation='sigmoid'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def _get_sae(inputs, hidden, output):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(hidden, input_dim=inputs, name='hidden'))\n",
        "    model.add(Activation('sigmoid'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(output, activation='sigmoid'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_saes(layers):\n",
        "\n",
        "    sae1 = _get_sae(layers[0], layers[1], layers[-1])\n",
        "    sae2 = _get_sae(layers[1], layers[2], layers[-1])\n",
        "    sae3 = _get_sae(layers[2], layers[3], layers[-1])\n",
        "\n",
        "    saes = Sequential()\n",
        "    saes.add(Dense(layers[1], input_dim=layers[0], name='hidden1'))\n",
        "    saes.add(Activation('sigmoid'))\n",
        "    saes.add(Dense(layers[2], name='hidden2'))\n",
        "    saes.add(Activation('sigmoid'))\n",
        "    saes.add(Dense(layers[3], name='hidden3'))\n",
        "    saes.add(Activation('sigmoid'))\n",
        "    saes.add(Dropout(0.2))\n",
        "    saes.add(Dense(layers[4], activation='sigmoid'))\n",
        "\n",
        "    models = [sae1, sae2, sae3, saes]\n",
        "\n",
        "    return models\n",
        "\n",
        "\n",
        "def process_data(train, test, lags):\n",
        "    attr = 'intensity'\n",
        "    \n",
        "    df1 = train\n",
        "    df2 = test\n",
        "    \n",
        "    df1['dateobservedfrom'] = pd.to_datetime(df1['dateobservedfrom'],dayfirst=True)\n",
        "    df2['dateobservedfrom'] = pd.to_datetime(df2['dateobservedfrom'],dayfirst=True) \n",
        "    \n",
        "    #set the date column as index\n",
        "    # Resample 30 min\n",
        "    \n",
        "    df1 = df1.set_index('dateobservedfrom').resample('30min').mean()\n",
        "    df2 = df2.set_index('dateobservedfrom').resample('30min').mean()\n",
        "    \n",
        "    # Polynomial Interpolation for NaN imputation\n",
        "    df1 = df1.interpolate('polynomial', order=1)\n",
        "    df2 = df2.interpolate('polynomial', order=1)\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1)).fit(df1[attr].values.reshape(-1, 1))\n",
        "    flow1 = scaler.transform(df1[attr].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
        "    flow2 = scaler.transform(df2[attr].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
        "\n",
        "    train, test = [], []\n",
        "    for i in range(lags, len(flow1)):\n",
        "        train.append(flow1[i - lags: i + 1])\n",
        "        #train.append(flow1[i - lags: i + 1])\n",
        "    for i in range(lags, len(flow2)):\n",
        "        test.append(flow2[i - lags: i + 1])\n",
        "\n",
        "    train = np.array(train)\n",
        "    test = np.array(test)\n",
        "    np.random.shuffle(train)\n",
        "\n",
        "    X_train = train[:, :-1]\n",
        "    y_train = train[:, -1]\n",
        "    X_test = test[:, :-1]\n",
        "    y_test = test[:, -1]\n",
        "\n",
        "    return X_train, y_train, X_test, y_test, scaler\n",
        "\n",
        "class Pogbar(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, sensor, model_name):\n",
        "        super(Pogbar, self).__init__()\n",
        "        self.epoch = 1\n",
        "        self.sensor = sensor\n",
        "        self.model_name = model_name\n",
        "        \n",
        "    def set_params(self, params):\n",
        "        self.epochs = params['epochs']\n",
        "        \n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        print(\"\\r{}: Training {} [{}/{}]\"\n",
        "                  .format(self.sensor, \n",
        "                          self.model_name, \n",
        "                          self.epoch, \n",
        "                          self.epochs), \n",
        "              end=\"\")\n",
        "        self.epoch += 1\n",
        "        \n",
        "    def on_train_end(self, logs=None):\n",
        "        print(\"\")\n",
        "        \n",
        "def train_model(sensor, model, X_train, y_train, name, config):\n",
        "    training_callbacks = [\n",
        "        Pogbar(sensor, name),\n",
        "        tf.keras.callbacks.CSVLogger(\"./logs/{}-{}.csv\".format(dt.datetime.now(), name), separator=\",\", append=False)\n",
        "    ]\n",
        "    \n",
        "    model.compile(loss=\"mse\", optimizer=\"rmsprop\", metrics=['mape'])\n",
        "    # early = EarlyStopping(monitor='val_loss', patience=30, verbose=0, mode='auto')\n",
        "    hist = model.fit(\n",
        "        X_train, y_train,\n",
        "        verbose=0,\n",
        "        batch_size=config[\"batch\"],\n",
        "        epochs=config[\"epochs\"],\n",
        "        validation_split=0.05,\n",
        "        callbacks=training_callbacks)\n",
        "\n",
        "    model.save('model/' + sensor + name + '.h5')\n",
        "    df = pd.DataFrame.from_dict(hist.history)\n",
        "    df.to_csv('model/' + sensor + name + ' loss.csv', encoding='utf-8', index=False)\n",
        "\n",
        "\n",
        "def train_saes(sensor, models, X_train, y_train, name, config):\n",
        "    training_callbacks = [\n",
        "        Pogbar(sensor, name),\n",
        "        tf.keras.callbacks.CSVLogger(\"./logs/{}-{}.csv\".format(dt.datetime.now(), name), separator=\",\", append=False)\n",
        "    ]\n",
        "    \n",
        "    temp = X_train\n",
        "    # early = EarlyStopping(monitor='val_loss', patience=30, verbose=0, mode='auto')\n",
        "\n",
        "    for i in range(len(models) - 1):\n",
        "        if i > 0:\n",
        "            p = models[i - 1]\n",
        "            hidden_layer_model = Model(p.input,\n",
        "                                       p.get_layer('hidden').output)\n",
        "            temp = hidden_layer_model.predict(temp)\n",
        "\n",
        "        m = models[i]\n",
        "        m.compile(loss=\"mse\", optimizer=\"rmsprop\", metrics=['mape'])\n",
        "\n",
        "        m.fit(temp, y_train, batch_size=config[\"batch\"],\n",
        "              epochs=config[\"epochs\"],\n",
        "              validation_split=0.05,\n",
        "              verbose=0,\n",
        "              callbacks=training_callbacks)\n",
        "\n",
        "        models[i] = m\n",
        "\n",
        "    saes = models[-1]\n",
        "    for i in range(len(models) - 1):\n",
        "        weights = models[i].get_layer('hidden').get_weights()\n",
        "        saes.get_layer('hidden%d' % (i + 1)).set_weights(weights)\n",
        "\n",
        "    train_model(sensor, saes, X_train, y_train, name, config)\n",
        "\n",
        "\n",
        "# come here to configure the model\n",
        "def train_main(sensor, train_data, test_data, config = {\"batch\": 256, \"epochs\": 30}):\n",
        "    lag = 12\n",
        "    X_train, y_train, _, _, _ = process_data(train_data, test_data, lag)\n",
        "    if os.path.isfile('model/' + sensor.split(\":\")[-1] + 'LSTM.h5'):\n",
        "        m = tf.keras.models.load_model('./model/' + sensor + 'LSTM.h5')\n",
        "    else:\n",
        "        X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "        m = get_lstm([12, 64, 64, 1])\n",
        "        train_model(sensor, m, X_train, y_train, \"LSTM\", config)\n",
        "    if os.path.isfile('model/' + sensor.split(\":\")[-1] + 'GRU.h5'):\n",
        "        m = tf.keras.models.load_model('./model/' + sensor + 'GRU.h5')\n",
        "    else:\n",
        "        X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "        m = get_gru([12, 64, 64, 1])\n",
        "        train_model(sensor, m, X_train, y_train, \"GRU\", config)\n",
        "    if os.path.isfile('model/' + sensor.split(\":\")[-1] + 'SAES.h5'):\n",
        "        m = tf.keras.models.load_model('./model/' + sensor + 'SAES.h5')\n",
        "    else:\n",
        "        X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1]))\n",
        "        m = get_saes([12, 400, 400, 400, 1])\n",
        "        train_saes(sensor, m, X_train, y_train, \"SAES\", config)\n",
        "\n",
        "        \n",
        "def MAPE(y_true, y_pred):\n",
        "    \n",
        "    y = [x for x in y_true if x > 0]\n",
        "    y_pred = [y_pred[i] for i in range(len(y_true)) if y_true[i] > 0]\n",
        "\n",
        "    num = len(y_pred)\n",
        "    sums = 0\n",
        "\n",
        "    for i in range(num):\n",
        "        tmp = abs(y[i] - y_pred[i]) / y[i]\n",
        "        sums += tmp\n",
        "\n",
        "    mape = sums * (100 / num)\n",
        "\n",
        "    return mape\n",
        "\n",
        "\n",
        "def eva_regress(name, y_true, y_pred):\n",
        "    mape = MAPE(y_true, y_pred)\n",
        "    vs = metrics.explained_variance_score(y_true, y_pred)\n",
        "    mae = metrics.mean_absolute_error(y_true, y_pred)\n",
        "    mse = metrics.mean_squared_error(y_true, y_pred)\n",
        "    r2 = metrics.r2_score(y_true, y_pred)\n",
        "    print(\"                           {}\".format(name))\n",
        "    print()\n",
        "    print('explained_variance_score:  %f' % vs)\n",
        "    print('                    mape:  %f%%' % mape)\n",
        "    print('                     mae:  %f' % mae)\n",
        "    print('                     mse:  %f' % mse)\n",
        "    print('                    rmse:  %f' % math.sqrt(mse))\n",
        "    print('                      r2:  %f' % r2)\n",
        "    print()\n",
        "\n",
        "\n",
        "def plot_results(y_true, y_preds, names):\n",
        "\n",
        "    d = '2016-12-31 00:00:00'\n",
        "    x = pd.date_range(d, periods=48, freq='30min') #######change according to frequency conversion of time series data\n",
        "\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "\n",
        "    ax.plot(x, y_true, label='True Data')\n",
        "    for name, y_pred in zip(names, y_preds):\n",
        "        ax.plot(x, y_pred, label=name)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.xlabel('Time of Day')\n",
        "    plt.ylabel('Flow')\n",
        "\n",
        "    date_format = mpl.dates.DateFormatter(\"%H:%M\")\n",
        "    ax.xaxis.set_major_formatter(date_format)\n",
        "    fig.autofmt_xdate()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def main(train_data, test_data, sensor):\n",
        "    lstm = load_model('model/' + sensor.split(\":\")[-1] + 'LSTM.h5')\n",
        "    gru = load_model('model/' + sensor.split(\":\")[-1] + 'GRU.h5')\n",
        "    saes = load_model('model/' + sensor.split(\":\")[-1] + 'SAES.h5')\n",
        "    models = [lstm, gru, saes]\n",
        "    names = ['LSTM', 'GRU', 'SAEs']\n",
        "\n",
        "    lag = 12\n",
        "    _, _, X_test, y_test, scaler = process_data(train_data, test_data, lag)\n",
        "    y_test = scaler.inverse_transform(y_test.reshape(-1, 1)).reshape(1, -1)[0]\n",
        "\n",
        "    y_preds = []\n",
        "    for name, model in zip(names, models):\n",
        "        if name == 'SAEs':\n",
        "            X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1]))\n",
        "        else:\n",
        "            X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "        file = 'images/' + name + '.png'\n",
        "        plot_model(model, to_file=file, show_shapes=True)\n",
        "        predicted = model.predict(X_test)\n",
        "        predicted = scaler.inverse_transform(predicted.reshape(-1, 1)).reshape(1, -1)[0]\n",
        "        y_preds.append(predicted[:48])\n",
        "        eva_regress(name, y_test, predicted)\n",
        "\n",
        "    plot_results(y_test[:48], y_preds, names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ90XveiBN6A"
      },
      "source": [
        "for (sensor, train_data),(_, test_data) in zip(train_df, test_df):\n",
        "    train_main(sensor.split(\":\")[-1], train_data, test_data, config={\"batch\": 256, \"epochs\": 60})\n",
        "    main(train_data, test_data, sensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSDath2nr1fq"
      },
      "source": [
        "## Conclusions\n",
        "\n",
        "### Scalability and Impact\n",
        "Tell us how applicable and scalable your solution is if you were to implement it in a city. Identify possible limitations and measure the potential social impact of your solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGmbES9GszEv"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XBiBOyAl2Sv"
      },
      "source": [
        "### Future Work\n",
        "Now picture the following scenario: imagine you could have access to any type of data that could help you solve this challenge even better. What would that data be and how would it improve your solution? üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gK3heTKl7qz"
      },
      "source": [
        ""
      ]
    }
  ]
}